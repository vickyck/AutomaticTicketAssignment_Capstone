{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ipywidgets python library\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings \n",
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer #word lemma class\n",
    "from deep_translator import GoogleTranslator # import google translator.\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Conv1D,Input, Bidirectional, LSTM, Dense, SpatialDropout1D, Input, Dropout,Embedding,Concatenate, TimeDistributed , BatchNormalization,Flatten,Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "lemma = WordNetLemmatizer()\n",
    "Gtrans= GoogleTranslator(source='auto', target='english')\n",
    "#initialize and download words,stopword and wordnet word corpus.\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from ipywidgets import Button, Layout, Box\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from sklearn.linear_model._base import _preprocess_data\n",
    "import _pickle as pickle\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for cleaning.\n",
    "def dataCleaner(text):\n",
    "    \n",
    "    #remove x000d from text.\n",
    "    text=text.replace('_x000D_',' ')\n",
    "    \n",
    "    #remove ? from text.\n",
    "    text=text.replace('?',' ')\n",
    "    \n",
    "    #correct acct to account.\n",
    "    text=text.replace('acct','account')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace('fw:','forward')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace(' fw ',' forward ')\n",
    "    \n",
    "    #Remove hex chars\n",
    "    lst = re.findall('==pcap 1 hex s==[\\w\\W]*==pcap 1 hex e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii chars\n",
    "    lst = re.findall('==pcap 1 ascii s==[\\w\\W]*==pcap 1 ascii e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove correlation data \n",
    "    lst = re.findall('\\[correlation_data\\](.*\\n){2}', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "       \n",
    "    #Remove [correlation_data]\n",
    "    lst = re.findall('\\[correlation_data\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove [no entry] \n",
    "    lst = re.findall('\\[no entry\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii packet(s):\n",
    "    text=text.replace('ascii packet(s):',' ')\n",
    "    \n",
    "    #Remove hex packet(s)::\n",
    "    text=text.replace('hex packet(s):',' ')\n",
    "        \n",
    "    # Removing white spaces and trimmign the space around words.\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    #removee tabs\n",
    "    text=text.replace('\\t',' ')\n",
    "    \n",
    "    text=text.replace('\\r',' ')\n",
    "    \n",
    "    #Remove new lines.\n",
    "    text=text.replace('\\n',' ')\n",
    "    \n",
    "    # We will use google translate to translate the sentence to english.\n",
    "    if(len(text)>4000):\n",
    "        n=3000\n",
    "        GtransStrings = []\n",
    "        for index in range(0, len(text), n):\n",
    "            separated_strings = text[index : index + n]\n",
    "            GtransStrings.append(Gtrans.translate(separated_strings))\n",
    "        text=\" \".join(GtransStrings)\n",
    "        \n",
    "    else:\n",
    "        if(text!=''):\n",
    "            text=Gtrans.translate(text)\n",
    "            \n",
    "    # Changing to lower case all the words.\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    # Removing english stop words.\n",
    "    text = [word.strip() for word in text if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # lemmatizing the words to their base forms\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Joining back the text.\n",
    "    text = ' '.join(text)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models, tokenizer, vectorizer & encoders saved\n",
    "def loadModelInfo(inp):\n",
    "    global model1, model2\n",
    "    global max_len, tokenizer, vectorizer, smallEncoder, FullEncoder\n",
    "    \n",
    "    model1 = keras.models.load_model('binaryLSTM_WithGTrans.h5')\n",
    "    model2 = pickle.load(open('./lr_model.sav', 'rb'))\n",
    "    max_len = 200\n",
    "    tokenizer = pickle.load(open('./tokenizer.sav', 'rb'))\n",
    "    vectorizer = pickle.load(open('./vectorizer.sav', 'rb'))\n",
    "    smallEncoder = pickle.load(open('./smallEncoder.sav', 'rb'))\n",
    "    FullEncoder = pickle.load(open('./FullEncoder.sav', 'rb'))\n",
    "    multiClass_Box.children[1].disabled = False\n",
    "    print(\"Model info loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions of the binary & the multi classification trained models to predict the assignment group \n",
    "# to which the ticket needs to be routed\n",
    "def getGroupAssignment(text):\n",
    "    text = dataCleaner(text)\n",
    "    if text.strip() == '' :\n",
    "        return 'GRP_0'\n",
    "    \n",
    "    X1 = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen = max_len)\n",
    "    model1Pred = FullEncoder.inverse_transform(np.round(model1.predict(X1),0).astype(int))\n",
    "    \n",
    "    if model1Pred == 'GRP_0':\n",
    "        return 'GRP_0'\n",
    "    else:\n",
    "        X2 = vectorizer.transform([text])\n",
    "        X2 = X2.toarray()\n",
    "        try:\n",
    "            model2Pred = smallEncoder.inverse_transform(model2.predict(X2))\n",
    "            model2Pred = model2Pred[0]\n",
    "        except:\n",
    "            model2Pred = 'GRP_0'\n",
    "        return model2Pred\n",
    "    \n",
    "    return 'GRP_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to collect user input, invoke the getGroupAssignment function, return the response & end the chat session\n",
    "def botOps():\n",
    "    try:\n",
    "        inp = input(\"Me: Please specify the issue \")\n",
    "        if inp.lower() == \"end\":\n",
    "            return \"exit\"\n",
    "        print(\"Bot: The ticket needs to be routed to this group:\", getGroupAssignment(inp))\n",
    "    \n",
    "    except EOFError as e:\n",
    "        print(e)\n",
    "        return \"exit\"        \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method to invoke the bot operation\n",
    "# Use the neural network pre-trained model to predict via the bag of words by \n",
    "# passing the input string entered by the user along with the words\n",
    "def startGLBot(self):\n",
    "    print(\"Welcome!! I'm the Great Learning Automatic Ticket Assignment Bot. How may I help you today? (Please type 'end' to exit support at any point of time)!!\")\n",
    "    temp = 0\n",
    "    resp = ''\n",
    "    while True:                \n",
    "        resp = botOps()        \n",
    "        if resp.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        try:            \n",
    "            inp = input(\"Bot: Was this helpful? Type Yes to exit and No to continue: \")\n",
    "            if inp.lower() == \"yes\":\n",
    "                print(\"It was a pleasure assisting you. Visit again. Good bye!!\")\n",
    "                break;\n",
    "            elif inp.lower() == \"no\":\n",
    "                botOps() \n",
    "            elif inp.lower() == \"end\":\n",
    "                print(\"It was a pleasure assisting you. Visit again. Good bye!!\")\n",
    "                break; \n",
    "            else:\n",
    "                print(\"Try again. Good bye!!\")\n",
    "                break;\n",
    "        except EOFError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the ipywidgets, building a simple UI to demonstrate the chatbot functionality\n",
    "\n",
    "def input_widget(text, callback):\n",
    "    label = widgets.Label(text)\n",
    "    \n",
    "    text = widgets.Text()\n",
    "    text.on_submit(callback)\n",
    "    box = widgets.HBox([label, text])\n",
    "    display(box)\n",
    "    \n",
    "def startChat(event=None):\n",
    "    clear_output(True)\n",
    "    print(\"Welcome!! I'm the Great Learning Automatic Ticket Assignment Bot. How may I help you today? \\n(Please type 'end' & hit 'enter' to exit support at any point of time)!!\")\n",
    "    input_widget(\"\", process)\n",
    "\n",
    "def process(event):\n",
    "    if event.value.lower() == \"end\" or event.value.lower() == \"exit\" or event.value.lower() == \"yes\":\n",
    "        clear_output(True)\n",
    "        label = widgets.Label(\"It was a pleasure assisting you. Visit again by re-executing the cell. Good bye!!\")\n",
    "        display(label)\n",
    "        return \"exit\"\n",
    "    \n",
    "    print(\"Bot: The ticket for this issue needs to be routed to the assignment group:\", getGroupAssignment(str(event.value)))\n",
    "    print(\"Bot: Was this helpful? Type Yes to exit and No to continue:\") \n",
    "    \n",
    "    if event.value.lower() == \"no\":\n",
    "        event.value = \"\"\n",
    "        startChat()\n",
    "        \n",
    "    event.value = \"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UI layout with only two buttons: Load Model & Chatbot Assistant\n",
    "items_layout = Layout(width='50%', height='80px') \n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    border='solid',\n",
    "                    width='100%')\n",
    "\n",
    "words = ['Load Composite Trained Model', 'Launch GL Auto Ticket Assignment Chatbot Assistant']\n",
    "items = [Button(description=word, layout=items_layout, button_style='danger', disabled=True) for word in words]\n",
    "global multiClass_Box\n",
    "multiClass_Box = Box(children=items, layout=box_layout)\n",
    "\n",
    "multiClass_Box.children[0].disabled = False\n",
    "multiClass_Box.children[0].on_click(loadModelInfo)\n",
    "multiClass_Box.children[1].on_click(startChat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info loaded successfully!\n",
      "Welcome!! I'm the Great Learning Automatic Ticket Assignment Bot. How may I help you today? (Please type 'end' to exit support at any point of time)!!\n",
      "Me: Please specify the issue exit\n",
      "Bot: The ticket needs to be routed to this group: GRP_0\n",
      "Bot: Was this helpful? Type Yes to exit and No to continue: yes\n",
      "It was a pleasure assisting you. Visit again. Good bye!!\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chat bot operation from the notebook console \n",
    "loadModelInfo('')\n",
    "startGLBot('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log only the errors & suppress warnings from tensorflow to improve readability\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ddd044b2b43858aaeac7f4a4fcf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Button(button_style='danger', description='Load Composite Trained Model', layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the UI for the chatbot\n",
    "multiClass_Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ddd044b2b43858aaeac7f4a4fcf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Button(button_style='danger', description='Load Composite Trained Model', layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiClass_Box"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
