{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-05T05:27:30.224065Z",
     "iopub.status.busy": "2022-02-05T05:27:30.223815Z",
     "iopub.status.idle": "2022-02-05T05:28:00.773217Z",
     "shell.execute_reply": "2022-02-05T05:28:00.772245Z",
     "shell.execute_reply.started": "2022-02-05T05:27:30.224037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (0.11.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (from torchvision) (8.3.1)\n",
      "Requirement already satisfied: torch==1.10.2 in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (from torchvision) (1.10.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (from torch==1.10.2->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\dell\\appdata\\roaming\\python\\python36\\site-packages (from torch==1.10.2->torchvision) (0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install openpyxl\n",
    "# !pip install deep_translator\n",
    "# !pip install NLPAug\n",
    "# !pip install gensim\n",
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:28:00.77557Z",
     "iopub.status.busy": "2022-02-05T05:28:00.775287Z",
     "iopub.status.idle": "2022-02-05T05:28:08.47217Z",
     "shell.execute_reply": "2022-02-05T05:28:08.471277Z",
     "shell.execute_reply.started": "2022-02-05T05:28:00.775535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer #word lemma class\n",
    "from deep_translator import GoogleTranslator # import google translator.\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Conv1D,Input, Bidirectional, LSTM, Dense, SpatialDropout1D, Input, Dropout,Embedding,Concatenate, TimeDistributed , BatchNormalization,Flatten,Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "lemma = WordNetLemmatizer()\n",
    "Gtrans= GoogleTranslator(source='auto', target='english')\n",
    "#initialize and download words,stopword and wordnet word corpus.\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:28:08.473749Z",
     "iopub.status.busy": "2022-02-05T05:28:08.473489Z",
     "iopub.status.idle": "2022-02-05T05:28:09.45987Z",
     "shell.execute_reply": "2022-02-05T05:28:09.459374Z",
     "shell.execute_reply.started": "2022-02-05T05:28:08.473724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Short description                                        Description  \\\n",
      "0       login issue  -verified user details.(employee# & manager na...   \n",
      "1           outlook  \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail...   \n",
      "\n",
      "              Caller Assignment group  \n",
      "0  spxjnwir pjlcoqds            GRP_0  \n",
      "1  hmjdrvpb komuaywn            GRP_0  \n"
     ]
    }
   ],
   "source": [
    "ticket=pd.read_excel('input_data automated ticket dataset.xlsx')\n",
    "print(ticket.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:28:09.461571Z",
     "iopub.status.busy": "2022-02-05T05:28:09.461303Z",
     "iopub.status.idle": "2022-02-05T05:28:09.474257Z",
     "shell.execute_reply": "2022-02-05T05:28:09.473622Z",
     "shell.execute_reply.started": "2022-02-05T05:28:09.461547Z"
    }
   },
   "outputs": [],
   "source": [
    "ticket['Description'].fillna(ticket['Short description'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:28:09.476413Z",
     "iopub.status.busy": "2022-02-05T05:28:09.475485Z",
     "iopub.status.idle": "2022-02-05T05:28:09.490426Z",
     "shell.execute_reply": "2022-02-05T05:28:09.489908Z",
     "shell.execute_reply.started": "2022-02-05T05:28:09.476377Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function for cleaning.\n",
    "def dataCleaner(text):\n",
    "    \n",
    "    #remove x000d from text.\n",
    "    text=text.replace('_x000D_',' ')\n",
    "    \n",
    "    #remove ? from text.\n",
    "    text=text.replace('?',' ')\n",
    "    \n",
    "    #correct acct to account.\n",
    "    text=text.replace('acct','account')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace('fw:','forward')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace(' fw ',' forward ')\n",
    "    \n",
    "    #remove (employee# & manager name)    \n",
    "    #text=text.replace('(employee# & manager name)',' ')\n",
    "        \n",
    "    #remove subject:    \n",
    "    #text=text.replace('subject:',' ')    \n",
    "    \n",
    "    #Remove received from: email\n",
    "   \n",
    "    \n",
    "    #Remove hex chars\n",
    "    lst = re.findall('==pcap 1 hex s==[\\w\\W]*==pcap 1 hex e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii chars\n",
    "    lst = re.findall('==pcap 1 ascii s==[\\w\\W]*==pcap 1 ascii e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove correlation data \n",
    "    lst = re.findall('\\[correlation_data\\](.*\\n){2}', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "       \n",
    "    #Remove [correlation_data]\n",
    "    lst = re.findall('\\[correlation_data\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove [no entry] \n",
    "    lst = re.findall('\\[no entry\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii packet(s):\n",
    "    text=text.replace('ascii packet(s):',' ')\n",
    "    \n",
    "    #Remove hex packet(s)::\n",
    "    text=text.replace('hex packet(s):',' ')\n",
    "        \n",
    "    # Removing white spaces and trimmign the space around words.\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    #removee tabs\n",
    "    text=text.replace('\\t',' ')\n",
    "    \n",
    "    text=text.replace('\\r',' ')\n",
    "    \n",
    "    #Remove new lines.\n",
    "    text=text.replace('\\n',' ')\n",
    "    \n",
    "    # We will use google translate to translate the sentence to english.\n",
    "    if(len(text)>4000):\n",
    "        n=3000\n",
    "        GtransStrings = []\n",
    "        for index in range(0, len(text), n):\n",
    "            separated_strings = text[index : index + n]\n",
    "            GtransStrings.append(Gtrans.translate(separated_strings))\n",
    "        text=\" \".join(GtransStrings)\n",
    "        \n",
    "    else:\n",
    "        if(text!=''):\n",
    "            text=Gtrans.translate(text)\n",
    "            \n",
    "    # Changing to lower case all the words.\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    # Removing english stop words.\n",
    "    text = [word.strip() for word in text if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # lemmatizing the words to their base forms\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Joining back the text.\n",
    "    text = ' '.join(text)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:28:09.491777Z",
     "iopub.status.busy": "2022-02-05T05:28:09.491332Z",
     "iopub.status.idle": "2022-02-05T05:48:13.06109Z",
     "shell.execute_reply": "2022-02-05T05:48:13.059981Z",
     "shell.execute_reply.started": "2022-02-05T05:28:09.491744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets clean all the text columns in the dataframe.\n",
    "# ticket['CleanedDescription'] = ticket['Description'].progress_apply(dataCleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.062261Z",
     "iopub.status.busy": "2022-02-05T05:48:13.062089Z",
     "iopub.status.idle": "2022-02-05T05:48:13.13514Z",
     "shell.execute_reply": "2022-02-05T05:48:13.134578Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.062238Z"
    }
   },
   "outputs": [],
   "source": [
    "# ticket.to_csv('./semicleaned2.csv',index=False)\n",
    "ticket = pd.read_csv('./semicleaned2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.136746Z",
     "iopub.status.busy": "2022-02-05T05:48:13.136385Z",
     "iopub.status.idle": "2022-02-05T05:48:13.14909Z",
     "shell.execute_reply": "2022-02-05T05:48:13.148065Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.136718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Short description      8\n",
       "Description            0\n",
       "Caller                 0\n",
       "Assignment group       0\n",
       "CleanedDescription    59\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticket.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.150566Z",
     "iopub.status.busy": "2022-02-05T05:48:13.150345Z",
     "iopub.status.idle": "2022-02-05T05:48:13.166047Z",
     "shell.execute_reply": "2022-02-05T05:48:13.165504Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.150535Z"
    }
   },
   "outputs": [],
   "source": [
    "ticket.drop(['Caller','Description','Short description'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.168869Z",
     "iopub.status.busy": "2022-02-05T05:48:13.168204Z",
     "iopub.status.idle": "2022-02-05T05:48:13.176502Z",
     "shell.execute_reply": "2022-02-05T05:48:13.176033Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.168836Z"
    }
   },
   "outputs": [],
   "source": [
    "ticket.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.17811Z",
     "iopub.status.busy": "2022-02-05T05:48:13.177465Z",
     "iopub.status.idle": "2022-02-05T05:48:13.189506Z",
     "shell.execute_reply": "2022-02-05T05:48:13.188457Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.178077Z"
    }
   },
   "outputs": [],
   "source": [
    "ticket['Group_Mod'] = ticket['Assignment group'].apply(lambda x: 'GRP_X' if x != 'GRP_0' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.191014Z",
     "iopub.status.busy": "2022-02-05T05:48:13.1906Z",
     "iopub.status.idle": "2022-02-05T05:48:13.213782Z",
     "shell.execute_reply": "2022-02-05T05:48:13.212763Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.190985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assignment group</th>\n",
       "      <th>CleanedDescription</th>\n",
       "      <th>Group_Mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRP_1</td>\n",
       "      <td>event: critical:hostname_221.company.com value...</td>\n",
       "      <td>GRP_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GRP_3</td>\n",
       "      <td>undocking pc , screen come back</td>\n",
       "      <td>GRP_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GRP_4</td>\n",
       "      <td>received from: kxsceyzo.naokumlb@gmail.com gen...</td>\n",
       "      <td>GRP_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>GRP_5</td>\n",
       "      <td>received from: yisohglr.uvteflgb@gmail.com hi ...</td>\n",
       "      <td>GRP_X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GRP_6</td>\n",
       "      <td>received from: monitoring_tool@company.com job...</td>\n",
       "      <td>GRP_X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Assignment group                                 CleanedDescription  \\\n",
       "6             GRP_1  event: critical:hostname_221.company.com value...   \n",
       "17            GRP_3                    undocking pc , screen come back   \n",
       "32            GRP_4  received from: kxsceyzo.naokumlb@gmail.com gen...   \n",
       "43            GRP_5  received from: yisohglr.uvteflgb@gmail.com hi ...   \n",
       "47            GRP_6  received from: monitoring_tool@company.com job...   \n",
       "\n",
       "   Group_Mod  \n",
       "6      GRP_X  \n",
       "17     GRP_X  \n",
       "32     GRP_X  \n",
       "43     GRP_X  \n",
       "47     GRP_X  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=ticket[ticket['Group_Mod']!='GRP_0'].copy()\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.215333Z",
     "iopub.status.busy": "2022-02-05T05:48:13.214958Z",
     "iopub.status.idle": "2022-02-05T05:48:13.219985Z",
     "shell.execute_reply": "2022-02-05T05:48:13.219272Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.215299Z"
    }
   },
   "outputs": [],
   "source": [
    "temp1=temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T05:48:13.221592Z",
     "iopub.status.busy": "2022-02-05T05:48:13.221046Z",
     "iopub.status.idle": "2022-02-05T05:48:27.980324Z",
     "shell.execute_reply": "2022-02-05T05:48:27.979449Z",
     "shell.execute_reply.started": "2022-02-05T05:48:13.221563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████        | 4006/4468 [00:10<00:01, 310.76it/s]"
     ]
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet', model_path=None, name='Synonym_Aug', aug_min=1, aug_max=10, aug_p=0.3,  \n",
    "                     stopwords=None, tokenizer=None, reverse_tokenizer=None, stopwords_regex=None, force_reload=False, \n",
    "                     verbose=0)\n",
    "temp1['CleanedDescription'] = temp1['CleanedDescription'].progress_apply(aug.augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.746277Z",
     "iopub.status.busy": "2022-02-05T06:04:22.745772Z",
     "iopub.status.idle": "2022-02-05T06:04:22.754027Z",
     "shell.execute_reply": "2022-02-05T06:04:22.753426Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.746235Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(temp))\n",
    "print(len(temp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.75946Z",
     "iopub.status.busy": "2022-02-05T06:04:22.757578Z",
     "iopub.status.idle": "2022-02-05T06:04:22.77005Z",
     "shell.execute_reply": "2022-02-05T06:04:22.769424Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.759423Z"
    }
   },
   "outputs": [],
   "source": [
    "newTemp1=pd.concat([temp,temp1])\n",
    "print(len(newTemp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.775548Z",
     "iopub.status.busy": "2022-02-05T06:04:22.773629Z",
     "iopub.status.idle": "2022-02-05T06:04:22.782133Z",
     "shell.execute_reply": "2022-02-05T06:04:22.781547Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.775512Z"
    }
   },
   "outputs": [],
   "source": [
    "XSmall=newTemp1['CleanedDescription']\n",
    "ySmall=newTemp1['Assignment group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.787549Z",
     "iopub.status.busy": "2022-02-05T06:04:22.785663Z",
     "iopub.status.idle": "2022-02-05T06:04:22.794114Z",
     "shell.execute_reply": "2022-02-05T06:04:22.793513Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.787514Z"
    }
   },
   "outputs": [],
   "source": [
    "print(XSmall.shape)\n",
    "print(ySmall.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.79571Z",
     "iopub.status.busy": "2022-02-05T06:04:22.7953Z",
     "iopub.status.idle": "2022-02-05T06:04:22.804171Z",
     "shell.execute_reply": "2022-02-05T06:04:22.803528Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.795657Z"
    }
   },
   "outputs": [],
   "source": [
    "XFull=ticket['CleanedDescription']\n",
    "yFull=ticket['Group_Mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.805734Z",
     "iopub.status.busy": "2022-02-05T06:04:22.805332Z",
     "iopub.status.idle": "2022-02-05T06:04:22.813561Z",
     "shell.execute_reply": "2022-02-05T06:04:22.812994Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.805683Z"
    }
   },
   "outputs": [],
   "source": [
    "print(XFull.shape)\n",
    "print(yFull.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:22.815149Z",
     "iopub.status.busy": "2022-02-05T06:04:22.814737Z",
     "iopub.status.idle": "2022-02-05T06:04:23.119276Z",
     "shell.execute_reply": "2022-02-05T06:04:23.118005Z",
     "shell.execute_reply.started": "2022-02-05T06:04:22.815115Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(ticket['CleanedDescription'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size :\", vocab_size)\n",
    "print(\"Word Indices: \", list(word_index.keys())[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.120714Z",
     "iopub.status.busy": "2022-02-05T06:04:23.120509Z",
     "iopub.status.idle": "2022-02-05T06:04:23.130474Z",
     "shell.execute_reply": "2022-02-05T06:04:23.129657Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.12067Z"
    }
   },
   "outputs": [],
   "source": [
    "FullEncoder=LabelEncoder()\n",
    "FullEncoder.fit(yFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.131856Z",
     "iopub.status.busy": "2022-02-05T06:04:23.131583Z",
     "iopub.status.idle": "2022-02-05T06:04:23.138595Z",
     "shell.execute_reply": "2022-02-05T06:04:23.137729Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.131823Z"
    }
   },
   "outputs": [],
   "source": [
    "#max_len = max(ticket['detailLen'])\n",
    "max_len=200\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.140151Z",
     "iopub.status.busy": "2022-02-05T06:04:23.139926Z",
     "iopub.status.idle": "2022-02-05T06:04:23.362557Z",
     "shell.execute_reply": "2022-02-05T06:04:23.361978Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.140122Z"
    }
   },
   "outputs": [],
   "source": [
    "XFull = pad_sequences(tokenizer.texts_to_sequences(XFull), maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.363973Z",
     "iopub.status.busy": "2022-02-05T06:04:23.363766Z",
     "iopub.status.idle": "2022-02-05T06:04:23.368881Z",
     "shell.execute_reply": "2022-02-05T06:04:23.368212Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.363948Z"
    }
   },
   "outputs": [],
   "source": [
    "yFull=FullEncoder.transform(yFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.370203Z",
     "iopub.status.busy": "2022-02-05T06:04:23.370015Z",
     "iopub.status.idle": "2022-02-05T06:04:23.380493Z",
     "shell.execute_reply": "2022-02-05T06:04:23.379583Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.370179Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:23.38466Z",
     "iopub.status.busy": "2022-02-05T06:04:23.384421Z",
     "iopub.status.idle": "2022-02-05T06:04:42.814638Z",
     "shell.execute_reply": "2022-02-05T06:04:42.813954Z",
     "shell.execute_reply.started": "2022-02-05T06:04:23.384635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate word embeddings\n",
    "# Construct the embedding weight matrix using the word embeddings via the tokenizer\n",
    "embeddings_index = dict()\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 15\n",
    "\n",
    "f = open('./glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:42.815796Z",
     "iopub.status.busy": "2022-02-05T06:04:42.815598Z",
     "iopub.status.idle": "2022-02-05T06:04:42.830307Z",
     "shell.execute_reply": "2022-02-05T06:04:42.829431Z",
     "shell.execute_reply.started": "2022-02-05T06:04:42.815773Z"
    }
   },
   "outputs": [],
   "source": [
    "Xf_train, Xf_test, yf_train, yf_test =  train_test_split(XFull, yFull, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:42.83193Z",
     "iopub.status.busy": "2022-02-05T06:04:42.831637Z",
     "iopub.status.idle": "2022-02-05T06:04:42.838253Z",
     "shell.execute_reply": "2022-02-05T06:04:42.837415Z",
     "shell.execute_reply.started": "2022-02-05T06:04:42.831897Z"
    }
   },
   "outputs": [],
   "source": [
    "#defining callbacks\n",
    "callbacks = [    \n",
    "    ModelCheckpoint('./model-{val_loss:.2f}.h5', monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=False),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:42.840059Z",
     "iopub.status.busy": "2022-02-05T06:04:42.839812Z",
     "iopub.status.idle": "2022-02-05T06:04:43.92972Z",
     "shell.execute_reply": "2022-02-05T06:04:43.928685Z",
     "shell.execute_reply.started": "2022-02-05T06:04:42.84003Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_DIM, weights = [embedding_matrix], input_length = max_len, trainable = False))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, input_shape=(None, 1))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#compile model\n",
    "LR = 0.01\n",
    "model.compile(loss = 'binary_crossentropy', optimizer=Adam(learning_rate = LR),metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:04:43.931683Z",
     "iopub.status.busy": "2022-02-05T06:04:43.931359Z",
     "iopub.status.idle": "2022-02-05T06:24:30.86583Z",
     "shell.execute_reply": "2022-02-05T06:24:30.86531Z",
     "shell.execute_reply.started": "2022-02-05T06:04:43.931647Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(Xf_train, yf_train, batch_size = 32, epochs = 10, validation_data = (Xf_test, yf_test), callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:24:30.867117Z",
     "iopub.status.busy": "2022-02-05T06:24:30.866852Z",
     "iopub.status.idle": "2022-02-05T06:24:30.873423Z",
     "shell.execute_reply": "2022-02-05T06:24:30.872729Z",
     "shell.execute_reply.started": "2022-02-05T06:24:30.867087Z"
    }
   },
   "outputs": [],
   "source": [
    "Xs_train, Xs_test, ys_train, ys_test =  train_test_split(XSmall, ySmall, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:24:30.874826Z",
     "iopub.status.busy": "2022-02-05T06:24:30.874488Z",
     "iopub.status.idle": "2022-02-05T06:25:10.934278Z",
     "shell.execute_reply": "2022-02-05T06:25:10.933443Z",
     "shell.execute_reply.started": "2022-02-05T06:24:30.8748Z"
    }
   },
   "outputs": [],
   "source": [
    "#initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5 ,use_idf=True,analyzer='word', token_pattern=r'\\w{1,}')\n",
    "# We will fit on whole data set do that there shouldnt be any case where a word is present in train set and not in test.\n",
    "vectorizer.fit(XSmall)\n",
    "Xs_train = vectorizer.transform(Xs_train)\n",
    "Xs_test = vectorizer.transform(Xs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:25:10.935735Z",
     "iopub.status.busy": "2022-02-05T06:25:10.93546Z",
     "iopub.status.idle": "2022-02-05T06:25:11.131212Z",
     "shell.execute_reply": "2022-02-05T06:25:11.130088Z",
     "shell.execute_reply.started": "2022-02-05T06:25:10.935684Z"
    }
   },
   "outputs": [],
   "source": [
    "Xs_train=Xs_train.toarray()\n",
    "Xs_test=Xs_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:25:11.132959Z",
     "iopub.status.busy": "2022-02-05T06:25:11.132724Z",
     "iopub.status.idle": "2022-02-05T06:25:11.140514Z",
     "shell.execute_reply": "2022-02-05T06:25:11.13979Z",
     "shell.execute_reply.started": "2022-02-05T06:25:11.132928Z"
    }
   },
   "outputs": [],
   "source": [
    "smallEncoder=LabelEncoder()\n",
    "smallEncoder.fit(ySmall)\n",
    "ys_train=smallEncoder.transform(ys_train)\n",
    "ys_test=smallEncoder.transform(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:25:11.141992Z",
     "iopub.status.busy": "2022-02-05T06:25:11.141739Z",
     "iopub.status.idle": "2022-02-05T06:28:57.211363Z",
     "shell.execute_reply": "2022-02-05T06:28:57.20798Z",
     "shell.execute_reply.started": "2022-02-05T06:25:11.141959Z"
    }
   },
   "outputs": [],
   "source": [
    "n_outputs = 1\n",
    "n_inputs = Xs_train.shape[1]\n",
    "model = Sequential()\n",
    "# We will keep input shape as 200, the size of our feature list.\n",
    "model.add(Dense(512, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "# compile model with loss as binary_crossentropy and metrics as accuracy\n",
    "model.compile(loss='binary_crossentropy', metrics = ['accuracy'],optimizer='adam')\n",
    "\n",
    "#Train/fit the model\n",
    "model.fit(Xs_train, ys_train, batch_size=8, epochs=25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T06:46:56.250105Z",
     "iopub.status.busy": "2022-02-05T06:46:56.249137Z",
     "iopub.status.idle": "2022-02-05T10:00:18.620529Z",
     "shell.execute_reply": "2022-02-05T10:00:18.616946Z",
     "shell.execute_reply.started": "2022-02-05T06:46:56.25005Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_lr = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100],'multi_class':['multinomial','auto']\n",
    "               ,'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag'],'n_jobs':[-1],'max_iter':[1000]}\n",
    "grid_lr = GridSearchCV(LogisticRegression() , param_grid_lr, cv = 2, n_jobs=-1, scoring='accuracy',verbose=1)\n",
    "# Training/Fitting Grid for Logistic Regression\n",
    "grid_lr.fit(Xs_train,ys_train)\n",
    "pickle.dump(grid_lr.best_estimator_, open('./lr_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.463431Z",
     "iopub.status.idle": "2022-02-05T06:28:59.463786Z",
     "shell.execute_reply": "2022-02-05T06:28:59.463608Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.463582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the best params which provide highest accuracy using different hyperparameter for Logistic regression model\n",
    "print(grid_lr.best_params_)\n",
    "print('LR accuracy on train data - ',grid_lr.score(Xs_train,ys_train))\n",
    "print('LR accuracy on test data - ',grid_lr.score(Xs_test,ys_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.465174Z",
     "iopub.status.idle": "2022-02-05T06:28:59.465462Z",
     "shell.execute_reply": "2022-02-05T06:28:59.465316Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.4653Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_nb = {} # Gaussian NB does not have hyper parameters to tune.\n",
    "grid_nb = GridSearchCV(GaussianNB() , param_grid_nb, cv = 2, n_jobs=-1, scoring='accuracy',verbose=2)\n",
    "# Training/Fitting Grid for NB\n",
    "grid_nb.fit(Xs_train,ys_train)\n",
    "pickle.dump(grid_nb.best_estimator_, open('./nb_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.466135Z",
     "iopub.status.idle": "2022-02-05T06:28:59.466435Z",
     "shell.execute_reply": "2022-02-05T06:28:59.466281Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.466266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the best params which provide highest accuracy using different hyperparameter for Logistic regression model\n",
    "print(grid_nb.best_params_)\n",
    "print('NB accuracy on train data - ',grid_nb.score(Xs_train,ys_train))\n",
    "print('NB accuracy on test data - ',grid_nb.score(Xs_test,ys_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.467923Z",
     "iopub.status.idle": "2022-02-05T06:28:59.468219Z",
     "shell.execute_reply": "2022-02-05T06:28:59.468074Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.468058Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_knn = {'n_neighbors': [2,3,5],'algorithm': ['auto','brute'], 'n_jobs':[-1]}\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier() , param_grid_knn, cv =2, n_jobs=-1, scoring='accuracy',verbose=2)\n",
    "# Training/Fitting Grid for KNN\n",
    "grid_knn.fit(Xs_train,ys_train)\n",
    "pickle.dump(grid_knn.best_estimator_, open('./knn_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.469487Z",
     "iopub.status.idle": "2022-02-05T06:28:59.46984Z",
     "shell.execute_reply": "2022-02-05T06:28:59.469658Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.469637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the best params which provide highest accuracy using different hyperparameter for Logistic regression model\n",
    "print(grid_knn.best_params_)\n",
    "print('KNN accuracy on train data - ',grid_knn.score(Xs_train,ys_train))\n",
    "print('KNN accuracy on test data - ',grid_knn.score(Xs_test,ys_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.470653Z",
     "iopub.status.idle": "2022-02-05T06:28:59.470981Z",
     "shell.execute_reply": "2022-02-05T06:28:59.470837Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.47082Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid_dt = {'criterion': ['gini','entropy'],'max_depth': [3,5,7], 'max_features':['auto','log2']}\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier() , param_grid_dt, cv = 2, n_jobs=-1, scoring='accuracy',verbose=2)\n",
    "# Training/Fitting Grid for DT\n",
    "grid_dt.fit(Xs_train,ys_train)\n",
    "pickle.dump(grid_dt.best_estimator_, open('./dt_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-02-05T06:28:59.472001Z",
     "iopub.status.idle": "2022-02-05T06:28:59.47229Z",
     "shell.execute_reply": "2022-02-05T06:28:59.472149Z",
     "shell.execute_reply.started": "2022-02-05T06:28:59.472133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the best params which provide highest accuracy using different hyperparameter for Logistic regression model\n",
    "print(grid_dt.best_params_)\n",
    "print('DT accuracy on train data - ',grid_dt.score(Xs_train,ys_train))\n",
    "print('DT accuracy on test data - ',grid_dt.score(Xs_test,ys_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('./tokenizer.sav', 'wb'))\n",
    "pickle.dump(vectorizer, open('./vectorizer.sav', 'wb'))\n",
    "pickle.dump(smallEncoder, open('./smallEncoder.sav', 'wb'))\n",
    "pickle.dump(FullEncoder, open('./FullEncoder.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.load_model('./model-0.37.h5')\n",
    "model2 = pickle.load(open('./lr_model.sav', 'rb'))\n",
    "max_len = 200\n",
    "tokenizer = pickle.load(open('./tokenizer.sav', 'rb'))\n",
    "vectorizer = pickle.load(open('./vectorizer.sav', 'rb'))\n",
    "smallEncoder = pickle.load(open('./smallEncoder.sav', 'rb'))\n",
    "FullEncoder = pickle.load(open('./FullEncoder.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupAssignment(text):\n",
    "    text=dataCleaner(text)\n",
    "    if text.strip() == '' :\n",
    "        return 'GRP_0'\n",
    "    X1=pad_sequences(tokenizer.texts_to_sequences([text]), maxlen = max_len)\n",
    "    model1Pred=FullEncoder.inverse_transform(np.round(model1.predict(X1),0).astype(int))\n",
    "    \n",
    "    if model1Pred=='GRP_0':\n",
    "        return 'GRP_0'\n",
    "    else:\n",
    "        X2=vectorizer.transform([text])\n",
    "        X2=X2.toarray()\n",
    "        try:\n",
    "            model2Pred=smallEncoder.inverse_transform(model2.predict(X2))\n",
    "            model2Pred=model2Pred[0]\n",
    "        except:\n",
    "            model2Pred = 'GRP_0'\n",
    "        return model2Pred\n",
    "    \n",
    "    return 'GRP_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function getGroupAssignment is used to predict for new text.\n",
    "getGroupAssignment('when undocking pc , screen will not come back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
