{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c41685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep_translator in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from deep_translator) (8.0.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from deep_translator) (4.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from deep_translator) (2.25.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.0.1->deep_translator) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\lib\\site-packages (from click<9.0.0,>=8.0.1->deep_translator) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (4.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep_translator) (3.4.1)\n",
      "Requirement already satisfied: openpyxl in c:\\programdata\\anaconda3\\lib\\site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in c:\\programdata\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install deep_translator\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a6eb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deshees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\deshees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\deshees\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer #word lemma class\n",
    "from deep_translator import GoogleTranslator # import google translator.\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Conv1D,Input, Bidirectional, LSTM, Dense, SpatialDropout1D, Input, Dropout,Embedding,Concatenate, TimeDistributed , BatchNormalization,Flatten,Activation\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "lemma = WordNetLemmatizer()\n",
    "Gtrans= GoogleTranslator(source='auto', target='english')\n",
    "#initialize and download words,stopword and wordnet word corpus.\n",
    "words = set(nltk.corpus.words.words())\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccee39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for cleaning.\n",
    "def dataCleaner(text):\n",
    "    \n",
    "    #remove x000d from text.\n",
    "    text=text.replace('_x000D_',' ')\n",
    "    \n",
    "    #remove ? from text.\n",
    "    text=text.replace('?',' ')\n",
    "    \n",
    "    #correct acct to account.\n",
    "    text=text.replace('acct','account')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace('fw:','forward')\n",
    "    \n",
    "    #correct fw to forward.\n",
    "    text=text.replace(' fw ',' forward ')\n",
    "    \n",
    "    #remove (employee# & manager name)    \n",
    "    #text=text.replace('(employee# & manager name)',' ')\n",
    "        \n",
    "    #remove subject:    \n",
    "    #text=text.replace('subject:',' ')    \n",
    "    \n",
    "    #Remove received from: email\n",
    "   \n",
    "    \n",
    "    #Remove hex chars\n",
    "    lst = re.findall('==pcap 1 hex s==[\\w\\W]*==pcap 1 hex e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii chars\n",
    "    lst = re.findall('==pcap 1 ascii s==[\\w\\W]*==pcap 1 ascii e==', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove correlation data \n",
    "    lst = re.findall('\\[correlation_data\\](.*\\n){2}', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "       \n",
    "    #Remove [correlation_data]\n",
    "    lst = re.findall('\\[correlation_data\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove [no entry] \n",
    "    lst = re.findall('\\[no entry\\]', text)\n",
    "    for i in lst:\n",
    "        text=text.replace(i,' ')\n",
    "        \n",
    "    #Remove ascii packet(s):\n",
    "    text=text.replace('ascii packet(s):',' ')\n",
    "    \n",
    "    #Remove hex packet(s)::\n",
    "    text=text.replace('hex packet(s):',' ')\n",
    "        \n",
    "    # Removing white spaces and trimmign the space around words.\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    #removee tabs\n",
    "    text=text.replace('\\t',' ')\n",
    "    \n",
    "    text=text.replace('\\r',' ')\n",
    "    \n",
    "    #Remove new lines.\n",
    "    text=text.replace('\\n',' ')\n",
    "    \n",
    "    # We will use google translate to translate the sentence to english.\n",
    "    if(len(text)>4000):\n",
    "        n=3000\n",
    "        GtransStrings = []\n",
    "        for index in range(0, len(text), n):\n",
    "            separated_strings = text[index : index + n]\n",
    "            GtransStrings.append(Gtrans.translate(separated_strings))\n",
    "        text=\" \".join(GtransStrings)\n",
    "        \n",
    "    else:\n",
    "        if(text!=''):\n",
    "            text=Gtrans.translate(text)\n",
    "            \n",
    "    # Changing to lower case all the words.\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    # Removing english stop words.\n",
    "    text = [word.strip() for word in text if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # lemmatizing the words to their base forms\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Joining back the text.\n",
    "    text = ' '.join(text)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "352e52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.load_model('./model-0.37.h5')\n",
    "model2 = pickle.load(open('./lr_model.sav', 'rb'))\n",
    "max_len = 200\n",
    "tokenizer = pickle.load(open('./tokenizer.sav', 'rb'))\n",
    "vectorizer = pickle.load(open('./vectorizer.sav', 'rb'))\n",
    "smallEncoder = pickle.load(open('./smallEncoder.sav', 'rb'))\n",
    "FullEncoder = pickle.load(open('./FullEncoder.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b5666ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupAssignment(text):\n",
    "    text=dataCleaner(text)\n",
    "    if text.strip() == '' :\n",
    "        return 'GRP_0'\n",
    "    X1=pad_sequences(tokenizer.texts_to_sequences([text]), maxlen = max_len)\n",
    "    model1Pred=FullEncoder.inverse_transform(np.round(model1.predict(X1),0).astype(int))\n",
    "    \n",
    "    if model1Pred=='GRP_0':\n",
    "        return 'GRP_0'\n",
    "    else:\n",
    "        X2=vectorizer.transform([text])\n",
    "        X2=X2.toarray()\n",
    "        try:\n",
    "            model2Pred=smallEncoder.inverse_transform(model2.predict(X2))\n",
    "            model2Pred=model2Pred[0]\n",
    "        except:\n",
    "            model2Pred = 'GRP_0'\n",
    "        return model2Pred\n",
    "    \n",
    "    return 'GRP_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7678e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GRP_3'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getGroupAssignment('when undocking pc , screen will not come back')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6a0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0666e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739b6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16bef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45474c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81760f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60db8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309a1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
